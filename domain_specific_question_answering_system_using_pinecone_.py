# -*- coding: utf-8 -*-
"""DOMAIN SPECIFIC QUESTION ANSWERING SYSTEM USING PINECONE .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O9b5kMZ_Guc-7xBUpoPE6To2JknZp-Uh
"""

!pip install pymupdf pdfminer.six langchain pinecone-client openai tiktoken sentence-transformers

from google.colab import files

uploaded = files.upload()  # Opens file upload dialog

for filename in uploaded.keys():
    print(f"Uploaded file: {filename}")

import fitz  # PyMuPDF

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text("text") + "\n"
    return text

# Extract text from the first uploaded file
pdf_filename = list(uploaded.keys())[0]  # Get uploaded file name
pdf_text = extract_text_from_pdf(pdf_filename)

print(pdf_text[:500])  # Print first 500 characters



!pip install nltk

import re
import nltk
nltk.download('punkt')

def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces & newlines
    text = re.sub(r'[^a-zA-Z0-9.,!?;:\s]', '', text)  # Remove special chars
    return text.strip()

cleaned_text = clean_text(pdf_text)
print(cleaned_text[:500])  # Print first 500 characters

import nltk

# First, ensure that 'punkt' is downloaded correctly
nltk.download('punkt_tab')  # Download the punkt tokenizer

# Import sentence tokenizer
from nltk.tokenize import sent_tokenize

def chunk_text(text, max_words=500):
    sentences = sent_tokenize(text)  # Tokenize the text into sentences
    chunks = []
    current_chunk = []
    word_count = 0

    for sentence in sentences:
        words = sentence.split()
        if word_count + len(words) > max_words:
            chunks.append(" ".join(current_chunk))  # Add current chunk to the list
            current_chunk = []  # Reset current chunk
            word_count = 0
        current_chunk.append(sentence)  # Add sentence to current chunk
        word_count += len(words)

    if current_chunk:
        chunks.append(" ".join(current_chunk))  # Add last chunk if available

    return chunks

# Example usage with 'cleaned_text' (replace this with actual cleaned text)
cleaned_text = "Your cleaned text goes here. It should be a long paragraph or document."

# Process the cleaned text into chunks
text_chunks = chunk_text(cleaned_text)

print(f"Total Chunks: {len(text_chunks)}")
print(text_chunks[0])  # Print first chunk



!pip install pinecone-client openai sentence-transformers



!pip uninstall -y pinecone-client

!pip install --upgrade pinecone

import os
from pinecone import Pinecone, ServerlessSpec

# Set your Pinecone API key
api_key = ""

# Initialize Pinecone
pc = Pinecone(api_key=api_key)

# Create a Pinecone index (if it doesn‚Äôt exist)
index_name = "database"

if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # Match your embedding model
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")  # Adjust if needed
    )

# Connect to the index
index = pc.Index(index_name)



import openai

openai.api_key = ""

def get_embedding(text):
    response = openai.embeddings.create(
        input=text,
        model="text-embedding-ada-002"
    )
    return response.data[0].embedding

# Example
sample_embedding = get_embedding(text_chunks[0])
print(len(sample_embedding))  # Should be 1536 dimensions

from uuid import uuid4  # Generate unique IDs

# Store all chunks in Pinecone
for chunk in text_chunks:
    vector = get_embedding(chunk)
    unique_id = str(uuid4())  # Generate a unique ID for each chunk
    index.upsert([(unique_id, vector, {"text": chunk})])  # Metadata helps in retrieval

print("All text chunks stored in Pinecone!")

!pip install langchain openai pinecone-client

!pip install -U langchain-community

import openai
import pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.schema import Document

import os
from pinecone import Pinecone, ServerlessSpec

# Initialize Pinecone
pc = Pinecone(api_key="")  # Use your actual API key

# Create index if not exists
index_name = "database"
if index_name not in pc.list_indexes().names():
    pc.create_index(
        name=index_name,
        dimension=1536,  # Make sure it matches your embedding model
        metric="cosine",  # You can use 'euclidean' or 'dotproduct' too
        spec=ServerlessSpec(
            cloud="aws",
            region="us-east-1"
        )
    )

# Connect to index
index = pc.Index(index_name)
print("Pinecone index is ready!")

from langchain.embeddings.openai import OpenAIEmbeddings

# Set API Key
OPENAI_API_KEY = ""

# Initialize OpenAI Embeddings
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

def retrieve_relevant_chunks(query, top_k=3):
    # Convert query to embedding
    query_embedding = embeddings.embed_query(query)

    # Search in Pinecone
    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)

    # Extract retrieved text
    retrieved_texts = [match['metadata']['text'] for match in results['matches']]
    return retrieved_texts

# Example query
query = "What are the key points in the document?"
retrieved_chunks = retrieve_relevant_chunks(query)

print("Relevant Chunks:", retrieved_chunks)

!pip install streamlit openai pinecone-client langchain
!pip install pyngrok  # Required to run Streamlit in Colab

import openai

# Set API Key
OPENAI_API_KEY = ""
client = openai.OpenAI(api_key=OPENAI_API_KEY)

def retrieve_relevant_chunks(query, top_k=3):
    # Convert query to embedding
    query_embedding = embeddings.embed_query(query)

    # Search in Pinecone
    results = index.query(vector=query_embedding, top_k=top_k, include_metadata=True)

    # Extract retrieved text
    retrieved_texts = [match['metadata']['text'] for match in results['matches']]

    return retrieved_texts

def generate_answer(query):
    # Retrieve relevant chunks from Pinecone
    retrieved_chunks = retrieve_relevant_chunks(query)

    # If no relevant chunks, return a default response
    if not retrieved_chunks:
        return "I couldn't find relevant information in the database."

    # Format retrieved text as context
    context = "\n".join(retrieved_chunks)

    # Construct the prompt for GPT-4
    prompt = f"""
    You are an AI assistant answering questions based on the provided context.

    Context:
    {context}

    Question: {query}

    Answer:
    """

    # Generate response using OpenAI GPT-4
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a helpful AI assistant."},
            {"role": "user", "content": prompt}
        ]
    )

    # Extract and return the answer
    answer = response.choices[0].message.content
    return answer

# Interactive Query
while True:
    query = input("\nüîπ Enter your question (or type 'exit' to stop): ")
    if query.lower() == "exit":
        break
    answer = generate_answer(query)
    print("\nüîπ Answer:", answer)

!pip install gradio



import gradio as gr

# üîß Make sure these are defined in your code
# - client: your OpenAI client instance
# - retrieve_relevant_chunks: function that retrieves relevant text from your Pinecone DB

def generate_answer(query):
    retrieved_chunks = retrieve_relevant_chunks(query)
    if not retrieved_chunks:
        return "‚ùó I couldn't find relevant information in the database."

    context = "\n".join(retrieved_chunks)
    prompt = f"""
    You are an AI assistant answering questions based on the provided context.

    Context:
    {context}

    Question: {query}

    Answer:
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "You are a helpful AI assistant."},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"‚ö†Ô∏è Error generating response: {str(e)}"

# üí° Use share=True for Colab to avoid connection error
with gr.Blocks(css="body {background-color: #f8f9fa;}") as demo:
    gr.Markdown("""
    # üìö Domain-Specific Question Answering System
    _Enter your query below and get an AI-generated response based on the uploaded document._
    """)

    with gr.Row():
        query = gr.Textbox(label="üîç Ask your question", placeholder="Type your question here...")

    with gr.Row():
        submit_btn = gr.Button("üöÄ Submit", variant="primary")

    output = gr.Textbox(label="üí¨ Response", interactive=False)

    submit_btn.click(fn=generate_answer, inputs=query, outputs=output)

# ‚úÖ This is the key fix for Google Colab
demo.launch(share=True)



